{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    '''\n",
    "    PyTorch implementation of the lookahead wrapper.\n",
    "    Lookahead Optimizer: https://arxiv.org/abs/1907.08610\n",
    "    '''\n",
    "\n",
    "    def __init__(self, optimizer, alpha=0.5, k=6, pullback_momentum=\"none\"):\n",
    "        '''\n",
    "        :param optimizer:inner optimizer\n",
    "        :param k (int): number of lookahead steps\n",
    "        :param alpha(float): linear interpolation factor. 1.0 recovers the inner optimizer.\n",
    "        :param pullback_momentum (str): change to inner optimizer momentum on interpolation update\n",
    "        '''\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        self.optimizer = optimizer\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.step_counter = 0\n",
    "        assert pullback_momentum in [\"reset\", \"pullback\", \"none\"]\n",
    "        self.pullback_momentum = pullback_momentum\n",
    "        self.state = defaultdict(dict)\n",
    "\n",
    "        # Cache the current optimizer parameters\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                param_state['cached_params'] = torch.zeros_like(p.data)\n",
    "                param_state['cached_params'].copy_(p.data)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return {\n",
    "            'state': self.state,\n",
    "            'optimizer': self.optimizer,\n",
    "            'alpha': self.alpha,\n",
    "            'step_counter': self.step_counter,\n",
    "            'k': self.k,\n",
    "            'pullback_momentum': self.pullback_momentum\n",
    "        }\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.optimizer.load_state_dict(state_dict)\n",
    "\n",
    "    def _backup_and_load_cache(self):\n",
    "        \"\"\"Useful for performing evaluation on the slow weights (which typically generalize better)\n",
    "        \"\"\"\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                param_state['backup_params'] = torch.zeros_like(p.data)\n",
    "                param_state['backup_params'].copy_(p.data)\n",
    "                p.data.copy_(param_state['cached_params'])\n",
    "\n",
    "    def _clear_and_load_backup(self):\n",
    "        for group in self.optimizer.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                p.data.copy_(param_state['backup_params'])\n",
    "                del param_state['backup_params']\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single Lookahead optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = self.optimizer.step(closure)\n",
    "        self.step_counter += 1\n",
    "\n",
    "        if self.step_counter >= self.k:\n",
    "            self.step_counter = 0\n",
    "            # Lookahead and cache the current optimizer parameters\n",
    "            for group in self.optimizer.param_groups:\n",
    "                for p in group['params']:\n",
    "                    param_state = self.state[p]\n",
    "                    p.data.mul_(self.alpha).add_(\n",
    "                        param_state['cached_params'], alpha=1.0 - self.alpha)  # fixed line\n",
    "\n",
    "                    param_state['cached_params'].copy_(p.data)\n",
    "                    if self.pullback_momentum == \"pullback\":\n",
    "                        internal_momentum = self.optimizer.state[p][\"momentum_buffer\"]\n",
    "                        self.optimizer.state[p][\"momentum_buffer\"] = internal_momentum.mul_(self.alpha).add_(\n",
    "                            1.0 - self.alpha, param_state[\"cached_mom\"])\n",
    "                        param_state[\"cached_mom\"] = self.optimizer.state[p][\"momentum_buffer\"]\n",
    "                    elif self.pullback_momentum == \"reset\":\n",
    "                        self.optimizer.state[p][\"momentum_buffer\"] = torch.zeros_like(\n",
    "                            p.data)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
    "                        p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * \\\n",
    "                        state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt(\n",
    "                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "                                N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    # p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    p_data_fp32.add_(\n",
    "                        other=-group['weight_decay'] * group['lr'], alpha=-1)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    # p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                    p_data_fp32.addcdiv_(\n",
    "                        tensor1=exp_avg, tensor2=denom, value=-step_size)\n",
    "\n",
    "                else:\n",
    "                    # p_data_fp32.add_(-step_size, exp_avg)\n",
    "                    p_data_fp32.add_(other=exp_avg, alpha=-step_size)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class PlainRAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "        super(PlainRAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(PlainRAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
    "                        p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                beta2_t = beta2 ** state['step']\n",
    "                N_sma_max = 2 / (1 - beta2) - 1\n",
    "                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay']\n",
    "                                     * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    step_size = group['lr'] * math.sqrt(\n",
    "                        (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n",
    "                            N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, warmup=warmup)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AdamW, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
    "                        p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                if group['warmup'] > state['step']:\n",
    "                    scheduled_lr = 1e-8 + \\\n",
    "                        state['step'] * group['lr'] / group['warmup']\n",
    "                else:\n",
    "                    scheduled_lr = group['lr']\n",
    "\n",
    "                step_size = group['lr'] * \\\n",
    "                    math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay']\n",
    "                                     * scheduled_lr, p_data_fp32)\n",
    "\n",
    "                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class CompoundProteinInteractionPrediction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompoundProteinInteractionPrediction, self).__init__()\n",
    "        self.embed_fingerprint = nn.Embedding(n_fingerprint, dim)\n",
    "        # self.embed_word = nn.Embedding(n_word, dim)\n",
    "        self.W_gnn1 = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                    for _ in range(layer_gnn)])\n",
    "        self.W_gnn2 = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                    for _ in range(layer_gnn)])\n",
    "        self.W_gnn3 = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                    for _ in range(layer_gnn)])\n",
    "        self.W_cnn = nn.ModuleList([nn.Conv2d(\n",
    "            in_channels=1, out_channels=1, kernel_size=3,\n",
    "            stride=1, padding=1) for _ in range(layer_gnn)])\n",
    "        self.w = nn.Linear(101, dim)\n",
    "        self.rnn = nn.LSTM(dim, dim, 1, bidirectional=True)\n",
    "        self.W_attention_compounds = nn.Linear(dim, dim)\n",
    "        self.W_attention_proteins = nn.Linear(2*dim, dim)\n",
    "        self.W_out = nn.ModuleList([nn.Linear(2*dim, 2*dim)\n",
    "                                    for _ in range(layer_output)])\n",
    "        self.W_interaction = nn.Linear(2*dim, 2)\n",
    "\n",
    "    def gat(self, xs, A, layer):\n",
    "        for i in range(layer):\n",
    "            hs1 = torch.relu(self.W_gnn1[i](xs))\n",
    "            hs2 = torch.relu(self.W_gnn2[i](xs))\n",
    "            weights = F.linear(hs1, hs2)\n",
    "            attn = weights.mul(A)\n",
    "            hs3 = torch.relu(self.W_gnn3[i](xs))\n",
    "            xs = xs + torch.matmul(attn, hs3)\n",
    "        return xs\n",
    "\n",
    "    def attention_rnn(self, x, xs, layer):\n",
    "        \"\"\"The attention mechanism is applied to the last layer of RNN.\"\"\"\n",
    "        xs = self.w(xs)\n",
    "        xs = torch.unsqueeze(xs, 1)\n",
    "        xs, _ = self.rnn(xs)\n",
    "        xs = torch.relu(xs)\n",
    "        xs = torch.squeeze(xs, 1)\n",
    "        h = torch.relu(self.W_attention_compounds(x))\n",
    "        hs = torch.relu(self.W_attention_proteins(xs))\n",
    "        weights = F.linear(h, hs)\n",
    "        weights_compounds = torch.mean(torch.t(weights), 0)\n",
    "        weights_compounds = torch.unsqueeze(weights_compounds, 0)\n",
    "        w_compounds = torch.tanh(weights_compounds)\n",
    "        weights_proteins = torch.mean(weights, 0)\n",
    "        weights_proteins = torch.unsqueeze(weights_proteins, 0)\n",
    "        w_proteins = torch.tanh(weights_proteins)\n",
    "        y = torch.t(weights_compounds) * h\n",
    "        ys = torch.t(weights_proteins) * hs\n",
    "        return torch.unsqueeze(torch.mean(ys, 0), 0), torch.unsqueeze(torch.mean(y, 0), 0), weights\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        fingerprints, adjacency, words = inputs\n",
    "        \"\"\"Compound vector with GAT.\"\"\"\n",
    "        fingerprint_vectors = self.embed_fingerprint(fingerprints)\n",
    "        compound_temp_vector = self.gat(\n",
    "            fingerprint_vectors, adjacency, layer_gnn)\n",
    "        \"\"\"Protein vector with attention-RNN.\"\"\"\n",
    "        # word_vectors = self.embed_word(words)\n",
    "        protein_vector, compound_vector, weights = self.attention_rnn(compound_temp_vector,\n",
    "                                                                      words, layer_cnn)\n",
    "        \"\"\"Concatenate the above two vectors and output the interaction.\"\"\"\n",
    "        cat_vector = torch.cat((compound_vector, protein_vector), 1)\n",
    "        for j in range(layer_output):\n",
    "            cat_vector = torch.relu(self.W_out[j](cat_vector))\n",
    "        interaction = self.W_interaction(cat_vector)\n",
    "        return interaction, weights\n",
    "\n",
    "    def __call__(self, data, train=True):\n",
    "        inputs, correct_interaction = data[:-1], data[-1]\n",
    "        predicted_interaction, weights = self.forward(inputs)\n",
    "        if train:\n",
    "            loss = F.cross_entropy(predicted_interaction, correct_interaction)\n",
    "            return loss\n",
    "        else:\n",
    "            correct_labels = correct_interaction.to('cpu').data.numpy()\n",
    "            ys = F.softmax(predicted_interaction, 1).to('cpu').data.numpy()\n",
    "            predicted_labels = list(map(lambda x: np.argmax(x), ys))\n",
    "            predicted_scores = list(map(lambda x: x[1], ys))\n",
    "            return correct_labels, predicted_labels, predicted_scores, weights\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        weight_p, bias_p = [], []\n",
    "\n",
    "        for p in self.model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                bias_p += [p]\n",
    "            else:\n",
    "                weight_p += [p]\n",
    "        self.optimizer_inner = RAdam(\n",
    "            [{'params': weight_p, 'weight_decay': weight_decay}, {'params': bias_p, 'weight_decay': weight_decay}], lr=lr)\n",
    "        self.optimizer = Lookahead(self.optimizer_inner, k=5, alpha=0.5)\n",
    "\n",
    "    def train(self, dataset):\n",
    "        np.random.shuffle(dataset)\n",
    "        N = len(dataset)\n",
    "        loss_total = 0\n",
    "        for data in dataset:\n",
    "            loss = self.model(data)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.to('cpu').data.numpy()\n",
    "        return loss_total\n",
    "\n",
    "\n",
    "class Tester(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def test(self, dataset):\n",
    "        N = len(dataset)\n",
    "        T, Y, S = [], [], []\n",
    "        for data in dataset:\n",
    "            (correct_labels, predicted_labels,\n",
    "             predicted_scores, _) = self.model(data, train=False)\n",
    "            T.append(correct_labels)\n",
    "            Y.append(predicted_labels)\n",
    "            S.append(predicted_scores)\n",
    "        AUC = roc_auc_score(T, S)\n",
    "        precision = precision_score(T, Y)\n",
    "        recall = recall_score(T, Y)\n",
    "        return AUC, precision, recall, T, Y, S\n",
    "\n",
    "    def save_AUCs(self, AUCs, filename):\n",
    "        with open(filename, 'a') as f:\n",
    "            f.write('\\t'.join(map(str, AUCs)) + '\\n')\n",
    "\n",
    "    def save_model(self, model, filename):\n",
    "        torch.save(model.state_dict(), filename)\n",
    "\n",
    "\n",
    "def load_tensor(file_name, dtype):\n",
    "    file = np.load(file_name + '.npy', allow_pickle=True)\n",
    "    new_file = []\n",
    "    for d in file:\n",
    "        new_file.append(d.astype(np.int64))\n",
    "    return [dtype(d).to(device) for d in new_file]\n",
    "\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def shuffle_dataset(dataset, seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset, ratio):\n",
    "    n = int(ratio * len(dataset))\n",
    "    dataset_1, dataset_2 = dataset[:n], dataset[n:]\n",
    "    return dataset_1, dataset_2\n",
    "\n",
    "\n",
    "def val(data, n_val, i):\n",
    "    l = int(len(data)/n_val)\n",
    "    if i == n_val-1:\n",
    "        val = data[(l*i):]\n",
    "        train = data[:(l*i)]\n",
    "    elif i == 0:\n",
    "        val = data[:l]\n",
    "        train = data[l:]\n",
    "    else:\n",
    "        val = data[(l*i):(l*(i+1))]\n",
    "        train = data[:(l*i)]+data[l*(i+1):]\n",
    "    return val, train\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(torch.cuda.is_available())\n",
    "    \"\"\"Hyperparameters.\"\"\"\n",
    "    DATASET, radius, ngram, dim, layer_gnn, window, layer_cnn, layer_output, lr, lr_decay, decay_interval, weight_decay, iteration = 'celegans', 2, 3, 128, 3, 11, 3, 3, 1e-3, 0.5, 10, 1e-6, 31\n",
    "    n_val = 5\n",
    "    setting = (DATASET + '_' + str(n_val) + '_' + str(dim) + '_' + str(layer_gnn) +\n",
    "               '_' + str(layer_output) + '_' + str(decay_interval) + '_' + str(weight_decay))\n",
    "    (dim, layer_gnn, window, layer_cnn, layer_output, decay_interval,\n",
    "     iteration) = map(int, [dim, layer_gnn, window, layer_cnn, layer_output,\n",
    "                            decay_interval, iteration])\n",
    "    lr, lr_decay, weight_decay = map(float, [lr, lr_decay, weight_decay])\n",
    "\n",
    "    \"\"\"CPU or GPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print('The code uses GPU...')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print('The code uses CPU!!!')\n",
    "\n",
    "    \"\"\"Output files.\"\"\"\n",
    "    file_AUCs = './output/result/' + '/AUCs_' + setting + '.txt'\n",
    "\n",
    "    AUCs = ('n_radius\\tn_val\\tEpoch\\tTime(sec)\\tLoss_train\\tAUC_dev\\t'\n",
    "            'AUC_test\\tPrecision_test\\tRecall_test')\n",
    "    with open(file_AUCs, 'w') as f:\n",
    "        f.write(AUCs + '\\n')\n",
    "    \"\"\"Start training.\"\"\"\n",
    "    print(AUCs)\n",
    "\n",
    "    \"\"\"Load preprocessed data.\"\"\"\n",
    "    dir_input = ('./input/' + DATASET + '/radius' +\n",
    "                 str(radius) + '_ngram' + str(ngram) + '/')\n",
    "    compounds = load_tensor(dir_input + 'compounds', torch.LongTensor)\n",
    "    adjacencies = load_tensor(dir_input + 'adjacencies', torch.FloatTensor)\n",
    "    proteins = load_tensor(dir_input + 'proteins', torch.FloatTensor)\n",
    "    interactions = load_tensor(dir_input + 'interactions', torch.LongTensor)\n",
    "    fingerprint_dict = load_pickle(dir_input + 'fingerprint_dict.pickle')\n",
    "    # word_dict = load_pickle(dir_input + 'word_dict.pickle')\n",
    "    n_fingerprint = len(fingerprint_dict)\n",
    "    # n_word = len(word_dict)\n",
    "    dataset = list(zip(compounds, adjacencies, proteins, interactions))\n",
    "    dataset = shuffle_dataset(dataset, 1234)\n",
    "    dataset_test, dataset_ = split_dataset(dataset, 0.1)\n",
    "    # dataset_test 1/10 and dataset_ 9/10\n",
    "    T_v, Y_v, S_v = torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
    "    for i in range(n_val):\n",
    "        file_model = './output/model/' + DATASET + '/' + \\\n",
    "            str(i) + '_' + str(radius) + '_' + setting\n",
    "        dataset_dev, dataset_train = val(dataset_, n_val, i)\n",
    "        torch.manual_seed(0)\n",
    "        model = CompoundProteinInteractionPrediction().to(device)\n",
    "        trainer = Trainer(model)\n",
    "        tester = Tester(model)\n",
    "        start = timeit.default_timer()\n",
    "        max_auc = 0\n",
    "        temp = 0\n",
    "        trainer.optimizer.param_groups[0]['lr'] = lr\n",
    "        for epoch in range(1, iteration):\n",
    "            loss_train = trainer.train(dataset_train)\n",
    "            AUC_dev = tester.test(dataset_dev)[0]\n",
    "            AUC_test, precision_test, recall_test, _, _, _ = tester.test(\n",
    "                dataset_test)\n",
    "            if temp > AUC_dev:\n",
    "                trainer.optimizer.param_groups[0]['lr'] *= lr_decay\n",
    "            end = timeit.default_timer()\n",
    "            time = end - start\n",
    "            AUCs = [radius, i, epoch, time, loss_train,\n",
    "                    AUC_dev, AUC_test, precision_test, recall_test]\n",
    "            tester.save_AUCs(AUCs, file_AUCs)\n",
    "            if AUC_dev > max_auc:\n",
    "                max_auc = AUC_dev\n",
    "                tester.save_model(model, file_model)\n",
    "            print('\\t'.join(map(str, AUCs)))\n",
    "            if np.abs(temp - AUC_dev) < 1e-4:\n",
    "                break\n",
    "            temp = AUC_dev\n",
    "        best_model = CompoundProteinInteractionPrediction().to(device)\n",
    "        best_model.load_state_dict(torch.load(file_model))\n",
    "        tester = Tester(best_model)\n",
    "        _, _, _, T, Y, S = tester.test(dataset_test)\n",
    "        T = torch.tensor(T)\n",
    "        T = torch.unsqueeze(T, 0)\n",
    "        Y = torch.tensor(Y)\n",
    "        Y = torch.unsqueeze(Y, 0)\n",
    "        S = torch.tensor(S)\n",
    "        S = torch.unsqueeze(S, 0)\n",
    "        T_v = torch.cat((T_v, T), dim=0)\n",
    "        Y_v = torch.cat((Y_v, Y), dim=0)\n",
    "        S_v = torch.cat((S_v, S), dim=0)\n",
    "    T_v = torch.squeeze(T_v, 2)\n",
    "    Y_v = torch.squeeze(Y_v, 2)\n",
    "    S_v = torch.squeeze(S_v, 2)\n",
    "    T = torch.mean(T_v, 0)\n",
    "    Y = torch.mean(S_v, 0)\n",
    "    Y = Y.round()\n",
    "    S = torch.mean(S_v, 0)\n",
    "    AUC = roc_auc_score(T, S)\n",
    "    precision = precision_score(T, Y)\n",
    "    recall = recall_score(T, Y)\n",
    "    AUCs = [AUC, precision, recall]\n",
    "    with open(file_AUCs, 'a') as f:\n",
    "        f.write('\\t'.join(map(str, AUCs)) + '\\n')\n",
    "    print('\\t'.join(map(str, AUCs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
